{"nbformat":4,"nbformat_minor":5,"metadata":{"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"name":"05_Useful Methods.ipynb","provenance":[],"collapsed_sections":["4cSov9jR0nJz"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"TpnuXo4A0nJp"},"source":["<div style=\"color:#006666; padding:0px 10px; border-radius:5px; font-size:18px;\"><h1 style='margin:10px 5px'>Useful Methods</h1>\n","</div>\n","\n","© Copyright Machine Learning Plus"],"id":"TpnuXo4A0nJp"},{"cell_type":"markdown","metadata":{"id":"tCc084br0nJp"},"source":["<div class=\"alert alert-info\" style=\"background-color:#006666; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'>1. Random Sampling and Shuffling</h2>\n","</div>"],"id":"tCc084br0nJp"},{"cell_type":"markdown","metadata":{"id":"Ct1XEaTE0nJq"},"source":["__Task__: How to pick random sample of observations from the dataframe? \n","\n","(Useful for performing cross validation experiments when building ML models)"],"id":"Ct1XEaTE0nJq"},{"cell_type":"code","metadata":{"id":"SKQ5yLNv0nJq"},"source":["import numpy as np\n","import pandas as pd"],"id":"SKQ5yLNv0nJq","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"tUIO5qy50nJq"},"source":["df = pd.read_csv(\"Datasets/Property_Crimes.csv\")\n","df.head(20)"],"id":"tUIO5qy50nJq","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4hSp0kn70nJq"},"source":["Randomly sample 70% of observations from `df` without replacement\n","\n","\n","`random_state` is for repeatability of the randomness."],"id":"4hSp0kn70nJq"},{"cell_type":"code","metadata":{"scrolled":false,"id":"IJYeBEtw0nJq"},"source":["nrows = round(df.shape[0]*.7)\n","df_sample = df.sample(nrows, replace=False, random_state=101)\n","df_sample"],"id":"IJYeBEtw0nJq","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M_J39lo80nJq"},"source":["### Bootstrap Sampling\n","\n","Bootstrap sampling is when you pick the same number of rows as in the dataset but with replacement. You can use `df.sample()` to easily do this.\n","\n","Used In:\n","1. In machine learning algorithms such as Random Forest, Bagging.\n","2. Computing confidence intervals"],"id":"M_J39lo80nJq"},{"cell_type":"code","metadata":{"id":"W04LDfAE0nJq"},"source":["df.sample(frac=1, replace=True, random_state=100)"],"id":"W04LDfAE0nJq","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dcyXDf9L0nJr"},"source":["### Challenge\n","\n","Calculate the 95% confidence interval of the means for the following:\n","\n","\n","```python\n","# Input\n","import numpy as np\n","np.random.seed(100)\n","arr = pd.Series(np.random.normal(10, 3, (100)))\n","arr.head()\n","```\n","\n","__Procedure:__\n","1. Bootstrap sample a large number of times (10000)\n","2. Calc mean of each sample.\n","3. Compute the 2.5%ile and 97.5%ile for the lower and upper bounds of the confidence intervals.\n"],"id":"dcyXDf9L0nJr"},{"cell_type":"code","metadata":{"id":"j52fifFX0nJr"},"source":["# Input\n","import numpy as np\n","np.random.seed(100)\n","arr = pd.Series(np.random.normal(10, 3, (100)))\n","arr.head()"],"id":"j52fifFX0nJr","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uz6ayBAR0nJr"},"source":["https://git.io/JsnsG"],"id":"uz6ayBAR0nJr","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-QKKdi260nJr"},"source":[""],"id":"-QKKdi260nJr","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XJ0lzjfa0nJr"},"source":[""],"id":"XJ0lzjfa0nJr","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DOhvUbWg0nJr"},"source":[""],"id":"DOhvUbWg0nJr","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W3xW6dXp0nJr"},"source":["# Solution\n","means = []\n","for i in range(10000):\n","    means.append(arr.sample(frac=1, replace=True).mean())\n","\n","# Sort\n","means = sorted(means)\n","means = pd.Series(means)\n","\n","# lower and upper\n","means.quantile(q=.025), means.quantile(q=.975)   "],"id":"W3xW6dXp0nJr","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2UgTMFDC0nJr"},"source":[""],"id":"2UgTMFDC0nJr","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2pJtHUjG0nJr"},"source":[""],"id":"2pJtHUjG0nJr","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Py8krONT0nJr"},"source":[""],"id":"Py8krONT0nJr","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ODc3NaQa0nJr"},"source":[""],"id":"ODc3NaQa0nJr","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RFv35qPx0nJr"},"source":[""],"id":"RFv35qPx0nJr","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oTiAwEwM0nJr"},"source":[""],"id":"oTiAwEwM0nJr","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d7cco80v0nJs"},"source":[""],"id":"d7cco80v0nJs","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zz2Dlef40nJs"},"source":[""],"id":"zz2Dlef40nJs","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VzeUU2r30nJs"},"source":[""],"id":"VzeUU2r30nJs","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EjrjF7hc0nJs"},"source":["<div class=\"alert alert-info\" style=\"background-color:#006666; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'>2. Dummy Variables</h2>\n","</div>"],"id":"EjrjF7hc0nJs"},{"cell_type":"markdown","metadata":{"id":"-j85BO500nJs"},"source":["Dummy variables, also called One-Hot Encoding is a way of converting a categorical variable into as many binary variables as there are categories.\n","\n","The reason to do this is to allow ML algorithms to understand and use the data."],"id":"-j85BO500nJs"},{"cell_type":"markdown","metadata":{"id":"lc_gaMwa0nJs"},"source":["![image.png](attachment:image.png)\n","Source: Stackoverflow"],"id":"lc_gaMwa0nJs"},{"cell_type":"code","metadata":{"id":"H5fCv3al0nJs"},"source":["import pandas as pd\n","df = pd.read_csv(\"Datasets/Property_Crimes.csv\")\n","df.head()"],"id":"H5fCv3al0nJs","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i8LBLo2U0nJs"},"source":["pd.get_dummies(df.Group_Name, prefix='Group')"],"id":"i8LBLo2U0nJs","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g-cCeh4p0nJs"},"source":[""],"id":"g-cCeh4p0nJs","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mwp3gQdN0nJs"},"source":[""],"id":"mwp3gQdN0nJs","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j0FuR5LM0nJs"},"source":[""],"id":"j0FuR5LM0nJs","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AuX2qBLU0nJs"},"source":[""],"id":"AuX2qBLU0nJs","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w8pwFo9r0nJs"},"source":[""],"id":"w8pwFo9r0nJs","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wNwSSCXm0nJs"},"source":[""],"id":"wNwSSCXm0nJs","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gux8Y-cA0nJs"},"source":[""],"id":"Gux8Y-cA0nJs","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ecqERfbx0nJt"},"source":[""],"id":"ecqERfbx0nJt","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L9u0_j1_0nJt"},"source":[" <div class=\"alert alert-info\" style=\"background-color:#006666; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'>3. Categorical Data</h2>\n","</div>"],"id":"L9u0_j1_0nJt"},{"cell_type":"markdown","metadata":{"id":"-Y72y-ej0nJt"},"source":["Pandas introduces a new memory efficient datatype for categorical data. It is used to \n","\n","1. __Represent a categorical variable__ present in string format. While a string column's (object dtype) memory allocation is dependent on a constant * length of the data, a categorical datatype's memory requirement depends in the number of categories. \n","\n","2. Indicate __intrinsic order__ within the categories (ordered data).\n","\n","3. __Signal other ML__ libraries, that this columns should be treated as categorical data. "],"id":"-Y72y-ej0nJt"},{"cell_type":"code","metadata":{"id":"jLfDh6N60nJt"},"source":["import pandas as pd\n","df = pd.read_csv(\"Datasets/Property_Crimes.csv\")\n","df.head()"],"id":"jLfDh6N60nJt","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rwpxWbdx0nJt"},"source":["df.info()"],"id":"rwpxWbdx0nJt","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZQVZjrVn0nJt"},"source":["__Task__: Convert the `Group_Name` column to a pandas categorical column"],"id":"ZQVZjrVn0nJt"},{"cell_type":"code","metadata":{"id":"b-0Sqe9D0nJt"},"source":["df['Group_Name_Cat'] = df['Group_Name'].astype('category')\n","df.head()"],"id":"b-0Sqe9D0nJt","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qncPPlvA0nJt"},"source":["df.info()"],"id":"qncPPlvA0nJt","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rb6D36WD0nJt"},"source":["__Check the datatype__"],"id":"rb6D36WD0nJt"},{"cell_type":"code","metadata":{"scrolled":true,"id":"BcAyT_U60nJt"},"source":["# Categorical\n","df['Group_Name_Cat'].dtype"],"id":"BcAyT_U60nJt","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FDiT7VTK0nJt"},"source":["# Object\n","df['Group_Name'].dtype"],"id":"FDiT7VTK0nJt","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6jK2AECU0nJt"},"source":["Alternately, you can use __`pd.categorical`__ to create the variable from scratch. "],"id":"6jK2AECU0nJt"},{"cell_type":"code","metadata":{"id":"LxdBrtTM0nJu"},"source":["cat = pd.Categorical(df['Group_Name'], categories=df['Group_Name'].unique())\n","cat"],"id":"LxdBrtTM0nJu","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ptoNZq1j0nJu"},"source":["Internally, the categories are stored as numerical codes, each code points to a specific category."],"id":"ptoNZq1j0nJu"},{"cell_type":"code","metadata":{"id":"DzwwYTCp0nJu"},"source":["cat.codes"],"id":"DzwwYTCp0nJu","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OibDgx1p0nJu"},"source":["See the categories"],"id":"OibDgx1p0nJu"},{"cell_type":"code","metadata":{"id":"9sJAzmRb0nJu"},"source":["cat.categories"],"id":"9sJAzmRb0nJu","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5lbxmj220nJu"},"source":["__Question__: Will result of `cat.categories` be always same as `cat.unique()`?\n","\n","Need not be. Because:\n","\n","1. `cat.unique()` will ensure the items will be listed in the order of appearance.\n","2. `cat.categories` may contain categories that are not actually part of data. Not the case with `cat.unique()`."],"id":"5lbxmj220nJu"},{"cell_type":"markdown","metadata":{"id":"pZsaxdxM0nJu"},"source":["If your data is a series, the categorical attributes are available under `series.cat.xyz`. Ex: `series.cat.categories`\n"],"id":"pZsaxdxM0nJu"},{"cell_type":"code","metadata":{"scrolled":true,"id":"RnClyADm0nJu"},"source":["ser = pd.Series(['a','b','c','d', 'a', 'b'], dtype=\"category\")\n","ser"],"id":"RnClyADm0nJu","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L-imkk0K0nJu"},"source":["# ser.categories  --> Wont work\n","ser.cat.categories"],"id":"L-imkk0K0nJu","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XOI-uyli0nJu"},"source":["__Add a new category__"],"id":"XOI-uyli0nJu"},{"cell_type":"code","metadata":{"scrolled":true,"id":"XY-a3KOM0nJu"},"source":["ser = ser.cat.add_categories('missing')\n","ser"],"id":"XY-a3KOM0nJu","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J0OW2AMD0nJv"},"source":["__Remove unused categories__\n","\n","It's possible that some cateogories may not actually be present in data."],"id":"J0OW2AMD0nJv"},{"cell_type":"code","metadata":{"id":"fdVfZCGL0nJv"},"source":["ser.cat.remove_unused_categories()"],"id":"fdVfZCGL0nJv","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YILLiMtM0nJv"},"source":["__Remove even used categories__"],"id":"YILLiMtM0nJv"},{"cell_type":"code","metadata":{"id":"WGROQTpO0nJv"},"source":["ser.cat.remove_categories('d')"],"id":"WGROQTpO0nJv","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6t2H4wCX0nJv"},"source":["__Ordered Categorical__ variables have an intrinsic ordering of the categories."],"id":"6t2H4wCX0nJv"},{"cell_type":"code","metadata":{"id":"qSeo04080nJv"},"source":["ser = pd.Series(pd.Categorical([\"a\", \"b\", \"c\", \"a\"], ordered=True))\n","ser"],"id":"qSeo04080nJv","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"lIMiKASX0nJv"},"source":["ser.cat.codes"],"id":"lIMiKASX0nJv","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6qz0AnO60nJv"},"source":["ser = ser.cat.reorder_categories(['c', 'b', 'a'], ordered=True)\n","ser"],"id":"6qz0AnO60nJv","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8OJx4Spx0nJv"},"source":["__Sorting__\n","\n","On sorting, the values will be rearranged as per the intrinsic ordering."],"id":"8OJx4Spx0nJv"},{"cell_type":"code","metadata":{"id":"eRrIxu8w0nJv"},"source":["ser.sort_values(inplace=True, ascending=False)\n","ser"],"id":"eRrIxu8w0nJv","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OqK0u8pl0nJv"},"source":[""],"id":"OqK0u8pl0nJv","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AIjmx52v0nJv"},"source":[""],"id":"AIjmx52v0nJv","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ipmH1CVU0nJv"},"source":[""],"id":"ipmH1CVU0nJv","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7lurfM5p0nJw"},"source":[""],"id":"7lurfM5p0nJw","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b3ssP5wt0nJw"},"source":[""],"id":"b3ssP5wt0nJw","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9h4LMZlm0nJw"},"source":[""],"id":"9h4LMZlm0nJw","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l23xTv7Q0nJw"},"source":[""],"id":"l23xTv7Q0nJw","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZUiVUQhh0nJw"},"source":[""],"id":"ZUiVUQhh0nJw","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8yI3UbnP0nJw"},"source":[""],"id":"8yI3UbnP0nJw","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CYG-R2qZ0nJw"},"source":[""],"id":"CYG-R2qZ0nJw","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_fJ563L-0nJw"},"source":["<div class=\"alert alert-info\" style=\"background-color:#006666; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'>4. Method Chaining</h2>\n","</div>"],"id":"_fJ563L-0nJw"},{"cell_type":"markdown","metadata":{"id":"QzWU2bt80nJw"},"source":["Method chaining is a programming style where you implement a pipeline of functions in a single call.\n","\n","__Advantage:__\n","1. Makes the code easy to read, in the sequence it happens.\n","2. Makes your codebase compact and organised.\n","\n","__Disadvantage:__\n","1. Harder to debug. "],"id":"QzWU2bt80nJw"},{"cell_type":"code","metadata":{"id":"DRs84YWr0nJw"},"source":["import pandas as pd\n","df = pd.read_csv(\"Datasets/Property_Crimes.csv\")\n","\n","# rename columns\n","df.columns = df.columns.str.lower()\n","\n","# recreate Subgroup\n","df['sub_group_name']  = df['group_name'].str.split(\"-\", expand=True).loc[:, 0]\n","\n","\n","# Convert Area_Name, Group_Name and Sub_Group_Name to categorical\n","df['area_name_cat']      = pd.Categorical(df['area_name']) \n","df['group_name_cat']     = pd.Categorical(df['group_name'])\n","df['sub_group_name_cat'] = pd.Categorical(df['sub_group_name'])\n","\n","\n","# Create Total Cases Lost\n","df['cases_lost'] = df['cases_property_stolen'] - df['cases_property_recovered']\n","\n","# Create Total Value of Lost\n","df['value_lost'] = df['value_of_property_stolen'] - df['value_of_property_recovered']\n","\n","df.sample(5)"],"id":"DRs84YWr0nJw","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"--sV5XkP0nJw"},"source":["Instead of having one call, you can pipe them all together in one function pipeline."],"id":"--sV5XkP0nJw"},{"cell_type":"code","metadata":{"id":"yURmoPQ10nJw"},"source":["def to_categorical(df, column_name):\n","    df[str(column_name)+'_Cat'] = pd.Categorical(df[column_name]) \n","    return df\n","\n","\n","def read_data(filepath):\n","    df = (pd.read_csv(filepath)\n","            .rename(columns=str.lower)\n","            .pipe(to_categorical, 'area_name')\n","            .pipe(to_categorical, 'group_name')\n","            .pipe(to_categorical, 'sub_group_name')\n","            .assign(cases_lost = lambda x: x['cases_property_stolen'] - x['cases_property_recovered'])\n","            .assign(value_lost = lambda x: x['value_of_property_stolen'] - x['value_of_property_recovered']))\n","    return df\n","\n","df = read_data(\"Datasets/Property_Crimes.csv\")\n","df.head()"],"id":"yURmoPQ10nJw","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3GbWwTtb0nJx"},"source":[""],"id":"3GbWwTtb0nJx","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KJFTVAgr0nJx"},"source":[""],"id":"KJFTVAgr0nJx","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"znanj7iJ0nJx"},"source":[""],"id":"znanj7iJ0nJx","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sVukj_au0nJx"},"source":[""],"id":"sVukj_au0nJx","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nXzP41ww0nJx"},"source":[""],"id":"nXzP41ww0nJx","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5UPZGrOU0nJx"},"source":[""],"id":"5UPZGrOU0nJx","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U9VBcaS70nJx"},"source":[""],"id":"U9VBcaS70nJx","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KKEojZW90nJx"},"source":[""],"id":"KKEojZW90nJx","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bQ3FEQ7Y0nJx"},"source":[" <div class=\"alert alert-info\" style=\"background-color:#006666; color:white; padding:0px 10px; border-radius:5px;\"><h2 style='margin:10px 5px'>5. Efficiently reading data from multiple CSV files</h2>\n","</div>"],"id":"bQ3FEQ7Y0nJx"},{"cell_type":"markdown","metadata":{"id":"I0O2uBKw0nJx"},"source":["__Task:__ \n","\n","Sometimes your data is not present in just one file but split in multiple files. And you want to read them all and combine into one single dataframe. \n","\n"],"id":"I0O2uBKw0nJx"},{"cell_type":"markdown","metadata":{"id":"r-vaU0cn0nJx"},"source":["__Approach 1:__"],"id":"r-vaU0cn0nJx"},{"cell_type":"code","metadata":{"id":"gnNiHf9W0nJy","outputId":"e27680a0-8c47-4ecf-a6a6-591468948d31"},"source":["import glob\n","csvfiles = []\n","\n","# provide path in relation to working directory or fill folder path\n","csvfiles = glob.glob(\"Datasets/AReM/lying\\*.csv\")\n","print(*csvfiles, sep=\"\\n\")\n"],"id":"gnNiHf9W0nJy","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Datasets/AReM/lying\\dataset1.csv\n","Datasets/AReM/lying\\dataset10.csv\n","Datasets/AReM/lying\\dataset11.csv\n","Datasets/AReM/lying\\dataset12.csv\n","Datasets/AReM/lying\\dataset13.csv\n","Datasets/AReM/lying\\dataset14.csv\n","Datasets/AReM/lying\\dataset15.csv\n","Datasets/AReM/lying\\dataset2.csv\n","Datasets/AReM/lying\\dataset3.csv\n","Datasets/AReM/lying\\dataset4.csv\n","Datasets/AReM/lying\\dataset5.csv\n","Datasets/AReM/lying\\dataset6.csv\n","Datasets/AReM/lying\\dataset7.csv\n","Datasets/AReM/lying\\dataset8.csv\n","Datasets/AReM/lying\\dataset9.csv\n"]}]},{"cell_type":"markdown","metadata":{"id":"0jXxRaMa0nJy"},"source":["__Read All files and Append to one Data Frame__"],"id":"0jXxRaMa0nJy"},{"cell_type":"code","metadata":{"id":"AHKRKd_V0nJy","outputId":"c00657c6-f3fd-407d-ea3b-982119b507de"},"source":["import os\n","import pandas as pd\n","\n","list_df = []\n","\n","\n","for csvfile in csvfiles:\n","    fpath = csvfile.replace(\"\\\\\",\"/\")\n","    print(\"Reading: \", fpath.ljust(40), \"Exists: \", os.path.exists(fpath))\n","    df = pd.read_csv(fpath, skiprows=4, header=0)\n","\n","    # Add filename column\n","    csv_name = csvfile.split('\\\\')[-1].split('.')[0]\n","    df['file'] = csv_name\n","    \n","    # Add df to a list\n","    list_df.append(df)\n","    \n","#concat all the df in the list\n","final_df = pd.concat(list_df)"],"id":"AHKRKd_V0nJy","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Reading:  Datasets/AReM/lying/dataset1.csv         Exists:  True\n","Reading:  Datasets/AReM/lying/dataset10.csv        Exists:  True\n","Reading:  Datasets/AReM/lying/dataset11.csv        Exists:  True\n","Reading:  Datasets/AReM/lying/dataset12.csv        Exists:  True\n","Reading:  Datasets/AReM/lying/dataset13.csv        Exists:  True\n","Reading:  Datasets/AReM/lying/dataset14.csv        Exists:  True\n","Reading:  Datasets/AReM/lying/dataset15.csv        Exists:  True\n","Reading:  Datasets/AReM/lying/dataset2.csv         Exists:  True\n","Reading:  Datasets/AReM/lying/dataset3.csv         Exists:  True\n","Reading:  Datasets/AReM/lying/dataset4.csv         Exists:  True\n","Reading:  Datasets/AReM/lying/dataset5.csv         Exists:  True\n","Reading:  Datasets/AReM/lying/dataset6.csv         Exists:  True\n","Reading:  Datasets/AReM/lying/dataset7.csv         Exists:  True\n","Reading:  Datasets/AReM/lying/dataset8.csv         Exists:  True\n","Reading:  Datasets/AReM/lying/dataset9.csv         Exists:  True\n"]}]},{"cell_type":"code","metadata":{"id":"djf-IjaL0nJy","outputId":"01e8c6f0-9c9f-4eee-a0b1-1f1b7d0a8bfe"},"source":["final_df.head()"],"id":"djf-IjaL0nJy","execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th># Columns: time</th>\n","      <th>avg_rss12</th>\n","      <th>var_rss12</th>\n","      <th>avg_rss13</th>\n","      <th>var_rss13</th>\n","      <th>avg_rss23</th>\n","      <th>var_rss23</th>\n","      <th>file</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>29.0</td>\n","      <td>0.0</td>\n","      <td>9.00</td>\n","      <td>0.71</td>\n","      <td>8.50</td>\n","      <td>0.50</td>\n","      <td>dataset1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>250</td>\n","      <td>29.0</td>\n","      <td>0.0</td>\n","      <td>8.00</td>\n","      <td>0.71</td>\n","      <td>8.50</td>\n","      <td>0.50</td>\n","      <td>dataset1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>500</td>\n","      <td>29.0</td>\n","      <td>0.0</td>\n","      <td>8.00</td>\n","      <td>0.71</td>\n","      <td>8.00</td>\n","      <td>1.00</td>\n","      <td>dataset1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>750</td>\n","      <td>28.5</td>\n","      <td>0.5</td>\n","      <td>8.25</td>\n","      <td>0.43</td>\n","      <td>8.75</td>\n","      <td>0.43</td>\n","      <td>dataset1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1000</td>\n","      <td>29.0</td>\n","      <td>0.0</td>\n","      <td>8.75</td>\n","      <td>1.09</td>\n","      <td>9.00</td>\n","      <td>0.00</td>\n","      <td>dataset1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   # Columns: time  avg_rss12  var_rss12  avg_rss13  var_rss13  avg_rss23  \\\n","0                0       29.0        0.0       9.00       0.71       8.50   \n","1              250       29.0        0.0       8.00       0.71       8.50   \n","2              500       29.0        0.0       8.00       0.71       8.00   \n","3              750       28.5        0.5       8.25       0.43       8.75   \n","4             1000       29.0        0.0       8.75       1.09       9.00   \n","\n","   var_rss23      file  \n","0       0.50  dataset1  \n","1       0.50  dataset1  \n","2       1.00  dataset1  \n","3       0.43  dataset1  \n","4       0.00  dataset1  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"H6M0tr2Z0nJy","outputId":"7118fbf5-b150-4a65-c1c5-b507bb8188eb"},"source":["final_df.shape"],"id":"H6M0tr2Z0nJy","execution_count":null,"outputs":[{"data":{"text/plain":["(7200, 8)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"qXfI0SZA0nJz"},"source":["df.columns"],"id":"qXfI0SZA0nJz","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ivhgcga10nJz"},"source":["__Approach 2: Generator Approach__\n","\n","Put it all in one function call."],"id":"ivhgcga10nJz"},{"cell_type":"code","metadata":{"scrolled":false,"id":"0AVQsBUd0nJz","outputId":"3b2d5773-6fa7-42c8-8ca4-db9e1d0f8b62"},"source":["df = pd.concat(pd.read_csv(fpath, skiprows=4, header=0) for fpath in csvfiles)\n","df"],"id":"0AVQsBUd0nJz","execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th># Columns: time</th>\n","      <th>avg_rss12</th>\n","      <th>var_rss12</th>\n","      <th>avg_rss13</th>\n","      <th>var_rss13</th>\n","      <th>avg_rss23</th>\n","      <th>var_rss23</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>29.00</td>\n","      <td>0.00</td>\n","      <td>9.00</td>\n","      <td>0.71</td>\n","      <td>8.50</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>250</td>\n","      <td>29.00</td>\n","      <td>0.00</td>\n","      <td>8.00</td>\n","      <td>0.71</td>\n","      <td>8.50</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>500</td>\n","      <td>29.00</td>\n","      <td>0.00</td>\n","      <td>8.00</td>\n","      <td>0.71</td>\n","      <td>8.00</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>750</td>\n","      <td>28.50</td>\n","      <td>0.50</td>\n","      <td>8.25</td>\n","      <td>0.43</td>\n","      <td>8.75</td>\n","      <td>0.43</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1000</td>\n","      <td>29.00</td>\n","      <td>0.00</td>\n","      <td>8.75</td>\n","      <td>1.09</td>\n","      <td>9.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>475</th>\n","      <td>118750</td>\n","      <td>41.50</td>\n","      <td>0.50</td>\n","      <td>10.67</td>\n","      <td>0.47</td>\n","      <td>14.00</td>\n","      <td>0.82</td>\n","    </tr>\n","    <tr>\n","      <th>476</th>\n","      <td>119000</td>\n","      <td>41.50</td>\n","      <td>0.50</td>\n","      <td>10.80</td>\n","      <td>0.40</td>\n","      <td>14.40</td>\n","      <td>0.80</td>\n","    </tr>\n","    <tr>\n","      <th>477</th>\n","      <td>119250</td>\n","      <td>41.75</td>\n","      <td>0.43</td>\n","      <td>10.00</td>\n","      <td>0.00</td>\n","      <td>13.67</td>\n","      <td>0.94</td>\n","    </tr>\n","    <tr>\n","      <th>478</th>\n","      <td>119500</td>\n","      <td>42.00</td>\n","      <td>0.00</td>\n","      <td>9.40</td>\n","      <td>0.49</td>\n","      <td>14.00</td>\n","      <td>1.10</td>\n","    </tr>\n","    <tr>\n","      <th>479</th>\n","      <td>119750</td>\n","      <td>41.75</td>\n","      <td>0.43</td>\n","      <td>9.00</td>\n","      <td>0.00</td>\n","      <td>14.33</td>\n","      <td>0.47</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7200 rows × 7 columns</p>\n","</div>"],"text/plain":["     # Columns: time  avg_rss12  var_rss12  avg_rss13  var_rss13  avg_rss23  \\\n","0                  0      29.00       0.00       9.00       0.71       8.50   \n","1                250      29.00       0.00       8.00       0.71       8.50   \n","2                500      29.00       0.00       8.00       0.71       8.00   \n","3                750      28.50       0.50       8.25       0.43       8.75   \n","4               1000      29.00       0.00       8.75       1.09       9.00   \n","..               ...        ...        ...        ...        ...        ...   \n","475           118750      41.50       0.50      10.67       0.47      14.00   \n","476           119000      41.50       0.50      10.80       0.40      14.40   \n","477           119250      41.75       0.43      10.00       0.00      13.67   \n","478           119500      42.00       0.00       9.40       0.49      14.00   \n","479           119750      41.75       0.43       9.00       0.00      14.33   \n","\n","     var_rss23  \n","0         0.50  \n","1         0.50  \n","2         1.00  \n","3         0.43  \n","4         0.00  \n","..         ...  \n","475       0.82  \n","476       0.80  \n","477       0.94  \n","478       1.10  \n","479       0.47  \n","\n","[7200 rows x 7 columns]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"DpGD6_M-0nJz"},"source":["If you care about adding the filename as a new column, define it in a function function."],"id":"DpGD6_M-0nJz"},{"cell_type":"code","metadata":{"id":"2HeLoO8Z0nJz"},"source":["def read(fpath):\n","    df = pd.read_csv(fpath, skiprows=4, header=0)\n","    csv_name = csvfile.split('/')[-1].split('.')[0]\n","    df['file'] = csv_name\n","    return df"],"id":"2HeLoO8Z0nJz","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"83KqZRZm0nJz","outputId":"1bba5834-61ed-402e-d407-832215039271"},"source":["df = pd.concat(read(fpath) for fpath in csvfiles)\n","df"],"id":"83KqZRZm0nJz","execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th># Columns: time</th>\n","      <th>avg_rss12</th>\n","      <th>var_rss12</th>\n","      <th>avg_rss13</th>\n","      <th>var_rss13</th>\n","      <th>avg_rss23</th>\n","      <th>var_rss23</th>\n","      <th>file</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>29.00</td>\n","      <td>0.00</td>\n","      <td>9.00</td>\n","      <td>0.71</td>\n","      <td>8.50</td>\n","      <td>0.50</td>\n","      <td>lying\\dataset9</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>250</td>\n","      <td>29.00</td>\n","      <td>0.00</td>\n","      <td>8.00</td>\n","      <td>0.71</td>\n","      <td>8.50</td>\n","      <td>0.50</td>\n","      <td>lying\\dataset9</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>500</td>\n","      <td>29.00</td>\n","      <td>0.00</td>\n","      <td>8.00</td>\n","      <td>0.71</td>\n","      <td>8.00</td>\n","      <td>1.00</td>\n","      <td>lying\\dataset9</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>750</td>\n","      <td>28.50</td>\n","      <td>0.50</td>\n","      <td>8.25</td>\n","      <td>0.43</td>\n","      <td>8.75</td>\n","      <td>0.43</td>\n","      <td>lying\\dataset9</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1000</td>\n","      <td>29.00</td>\n","      <td>0.00</td>\n","      <td>8.75</td>\n","      <td>1.09</td>\n","      <td>9.00</td>\n","      <td>0.00</td>\n","      <td>lying\\dataset9</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>475</th>\n","      <td>118750</td>\n","      <td>41.50</td>\n","      <td>0.50</td>\n","      <td>10.67</td>\n","      <td>0.47</td>\n","      <td>14.00</td>\n","      <td>0.82</td>\n","      <td>lying\\dataset9</td>\n","    </tr>\n","    <tr>\n","      <th>476</th>\n","      <td>119000</td>\n","      <td>41.50</td>\n","      <td>0.50</td>\n","      <td>10.80</td>\n","      <td>0.40</td>\n","      <td>14.40</td>\n","      <td>0.80</td>\n","      <td>lying\\dataset9</td>\n","    </tr>\n","    <tr>\n","      <th>477</th>\n","      <td>119250</td>\n","      <td>41.75</td>\n","      <td>0.43</td>\n","      <td>10.00</td>\n","      <td>0.00</td>\n","      <td>13.67</td>\n","      <td>0.94</td>\n","      <td>lying\\dataset9</td>\n","    </tr>\n","    <tr>\n","      <th>478</th>\n","      <td>119500</td>\n","      <td>42.00</td>\n","      <td>0.00</td>\n","      <td>9.40</td>\n","      <td>0.49</td>\n","      <td>14.00</td>\n","      <td>1.10</td>\n","      <td>lying\\dataset9</td>\n","    </tr>\n","    <tr>\n","      <th>479</th>\n","      <td>119750</td>\n","      <td>41.75</td>\n","      <td>0.43</td>\n","      <td>9.00</td>\n","      <td>0.00</td>\n","      <td>14.33</td>\n","      <td>0.47</td>\n","      <td>lying\\dataset9</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7200 rows × 8 columns</p>\n","</div>"],"text/plain":["     # Columns: time  avg_rss12  var_rss12  avg_rss13  var_rss13  avg_rss23  \\\n","0                  0      29.00       0.00       9.00       0.71       8.50   \n","1                250      29.00       0.00       8.00       0.71       8.50   \n","2                500      29.00       0.00       8.00       0.71       8.00   \n","3                750      28.50       0.50       8.25       0.43       8.75   \n","4               1000      29.00       0.00       8.75       1.09       9.00   \n","..               ...        ...        ...        ...        ...        ...   \n","475           118750      41.50       0.50      10.67       0.47      14.00   \n","476           119000      41.50       0.50      10.80       0.40      14.40   \n","477           119250      41.75       0.43      10.00       0.00      13.67   \n","478           119500      42.00       0.00       9.40       0.49      14.00   \n","479           119750      41.75       0.43       9.00       0.00      14.33   \n","\n","     var_rss23            file  \n","0         0.50  lying\\dataset9  \n","1         0.50  lying\\dataset9  \n","2         1.00  lying\\dataset9  \n","3         0.43  lying\\dataset9  \n","4         0.00  lying\\dataset9  \n","..         ...             ...  \n","475       0.82  lying\\dataset9  \n","476       0.80  lying\\dataset9  \n","477       0.94  lying\\dataset9  \n","478       1.10  lying\\dataset9  \n","479       0.47  lying\\dataset9  \n","\n","[7200 rows x 8 columns]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"4cSov9jR0nJz"},"source":["### Challenge"],"id":"4cSov9jR0nJz"},{"cell_type":"markdown","metadata":{"id":"BGZ2DAB60nJz"},"source":["__1.__ Read only the following two columns ('avg_rss12', 'var_rss12') and the first 10 observations from each file in `Datasets/AReM/lying` directory and append as one dataframe `df`.\n","\n","__2.__ Then, re-distribute and store this df into 15 separate csv files."],"id":"BGZ2DAB60nJz"}]}